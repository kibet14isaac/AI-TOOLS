# -*- coding: utf-8 -*-
"""DEEP LEARNING.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1mh6EnHmFUmazWVHeoboQibHbd6krJTWu
"""

import tensorflow as tf
from tensorflow.keras.datasets import mnist
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout
from tensorflow.keras.utils import to_categorical
import matplotlib.pyplot as plt
import numpy as np

# Load the MNIST dataset
(X_train, y_train), (X_test, y_test) = mnist.load_data()

print(f"Original X_train shape: {X_train.shape}")
print(f"Original y_train shape: {y_train.shape}")
print(f"Original X_test shape: {X_test.shape}")
print(f"Original y_test shape: {y_test.shape}")

# Preprocess the data
# Reshape data to include a channel dimension (for grayscale images, it's 1)
# TensorFlow expects (batch_size, height, width, channels)
X_train = X_train.reshape(X_train.shape[0], 28, 28, 1).astype('float32')
X_test = X_test.reshape(X_test.shape[0], 28, 28, 1).astype('float32')

# Normalize pixel values to be between 0 and 1
X_train /= 255
X_test /= 255

# Convert labels to one-hot encoding
# e.g., 5 becomes [0, 0, 0, 0, 0, 1, 0, 0, 0, 0]
num_classes = 10
y_train = to_categorical(y_train, num_classes)
y_test = to_categorical(y_test, num_classes)

print(f"\nPreprocessed X_train shape: {X_train.shape}")
print(f"Preprocessed y_train shape: {y_train.shape}")
print(f"Preprocessed X_test shape: {X_test.shape}")
print(f"Preprocessed y_test shape: {y_test.shape}")


# Build the CNN model
model = Sequential([
    # Convolutional Layer 1
    Conv2D(32, kernel_size=(3, 3), activation='relu', input_shape=(28, 28, 1)),
    MaxPooling2D(pool_size=(2, 2)),
    Dropout(0.25), # Regularization to prevent overfitting

    # Convolutional Layer 2
    Conv2D(64, (3, 3), activation='relu'),
    MaxPooling2D(pool_size=(2, 2)),
    Dropout(0.25),

    # Flatten the output for the Dense layers
    Flatten(),

    # Fully Connected Layers
    Dense(128, activation='relu'),
    Dropout(0.5), # More dropout for the dense layers
    Dense(num_classes, activation='softmax') # Output layer with 10 units for 10 digits
])

# Compile the model
model.compile(optimizer='adam',
              loss='categorical_crossentropy',
              metrics=['accuracy'])

model.summary()

# Train the model
# Using a callback for early stopping to prevent overfitting and save training time
early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)

history = model.fit(X_train, y_train,
                    batch_size=128,
                    epochs=20, # Increased epochs, but early stopping will manage it
                    verbose=1,
                    validation_split=0.1, # Use a validation split from the training data
                    callbacks=[early_stopping])

# Evaluate the model
loss, accuracy = model.evaluate(X_test, y_test, verbose=0)
print(f"\n--- Model Evaluation on Test Set ---")
print(f"Test Loss: {loss:.4f}")
print(f"Test Accuracy: {accuracy:.4f}")

# Check if accuracy goal is met
if accuracy > 0.95:
    print("Achieved >95% test accuracy! Well done!")
else:
    print(f"Accuracy is {accuracy*100:.2f}%, which is below 95%. Consider adjusting hyperparameters or model architecture.")

# Visualize the modelâ€™s predictions on 5 sample images
print("\nVisualizing predictions on 5 sample images:")
sample_indices = np.random.choice(X_test.shape[0], 5, replace=False)
sample_images = X_test[sample_indices]
sample_true_labels = np.argmax(y_test[sample_indices], axis=1) # Convert one-hot to integer
sample_predictions = model.predict(sample_images)
sample_predicted_labels = np.argmax(sample_predictions, axis=1)

plt.figure(figsize=(12, 5))
for i in range(5):
    plt.subplot(1, 5, i + 1)
    # Reshape back to 28x28 for displaying
    plt.imshow(sample_images[i].reshape(28, 28), cmap='gray')
    plt.title(f"True: {sample_true_labels[i]}\nPred: {sample_predicted_labels[i]}")
    plt.axis('off')
plt.tight_layout()
plt.show()



