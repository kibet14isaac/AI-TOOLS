# -*- coding: utf-8 -*-
"""Classical ML with Scikit-learn.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1QQqTkcLnMWzwOykvn_7vLlqhahkTsrjb
"""

import pandas as pd
from sklearn.datasets import load_iris
from sklearn.model_selection import train_test_split
from sklearn.tree import DecisionTreeClassifier
from sklearn.metrics import accuracy_score, precision_score, recall_score
from sklearn.preprocessing import LabelEncoder

# Load the Iris dataset
iris = load_iris()
X = pd.DataFrame(iris.data, columns=iris.feature_names)
y = pd.Series(iris.target)

print("Original Data (first 5 rows):")
print(X.head())
print("\nOriginal Target Labels (first 5):")
print(y.head())

# 1. Preprocess the data

# Handle missing values (Iris dataset is clean, but demonstrating the step)
# For numerical features, you might use: X.fillna(X.mean(), inplace=True)
# For categorical features, you might use: X.fillna(X.mode().iloc[0], inplace=True)
# In this case, there are no missing values in the Iris dataset.
print("\nChecking for missing values:")
print(X.isnull().sum())
print(y.isnull().sum())

# Encode labels (species names are already numerical in load_iris, but if they were strings)
# If y were strings like 'setosa', 'versicolor', 'virginica', we would do:
# le = LabelEncoder()
# y_encoded = le.fit_transform(y)
# Since iris.target is already numerical (0, 1, 2), no explicit encoding is needed here for the target.
# If features had categorical columns, we'd use OneHotEncoder or LabelEncoder on X.

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

print(f"\nTraining set shape: {X_train.shape}, {y_train.shape}")
print(f"Testing set shape: {X_test.shape}, {y_test.shape}")

# 2. Train a Decision Tree Classifier
dt_classifier = DecisionTreeClassifier(random_state=42)
dt_classifier.fit(X_train, y_train)

print("\nDecision Tree Classifier trained successfully.")

# 3. Evaluate using accuracy, precision, and recall
y_pred = dt_classifier.predict(X_test)

accuracy = accuracy_score(y_test, y_pred)
# For precision and recall, it's often useful to specify average for multi-class classification
# 'weighted' accounts for class imbalance. 'macro' calculates metrics for each label and finds their unweighted mean.
# 'micro' calculates metrics globally by counting the total true positives, false negatives and false positives.
precision = precision_score(y_test, y_pred, average='weighted')
recall = recall_score(y_test, y_pred, average='weighted')

print(f"\n--- Model Evaluation ---")
print(f"Accuracy: {accuracy:.4f}")
print(f"Precision (weighted): {precision:.4f}")
print(f"Recall (weighted): {recall:.4f}")

# You can also see the classification report for more detailed metrics per class
from sklearn.metrics import classification_report
print("\nClassification Report:")
print(classification_report(y_test, y_pred, target_names=iris.target_names))